![[Pasted image 20251220150258.png]]

![[Pasted image 20251220154257.png]]
#### Virtual Memory
虚拟地址是做什么的？
隔离：阻止进程间相互干扰，让每个进程都认为自己占有全部的资源。
CPU正在执行指令，例如sd $7, (a0)。对于任何一条带有地址的指令，其中的地址应该认为是虚拟内存地址而不是物理地址。假设寄存器a0中是地址0x1000，那么这是一个虚拟内存地址。虚拟内存地址会被转到内存管理单元MMU内存管理单元，翻译成物理地址。

**虚拟地址位 = VPN位 + Offset位**，**物理地址位 = PPN位 + Offset位**。
Offset取决于Page的大小，VPN位取决于实际地址存储位数，PPN位取决于实际空间容量。
例如：地址32bit，pagesize 4096byte（$2^{12}$），物理内存地址空间大小1Mb
offset = 12位，VPN = 32-12位，PPN = 20-12位
page table有$2^{20}$条，（实际不会这样设计）

下面讲的是page table，注意别跟page size搞混了。另外内容比较拓展，大致了解即可。
>这是个非常大的数字。如果每个进程都使用这么大的page table，进程需要为page table消耗大量的内存，并且很快物理内存就会耗尽。
>所以实际上，硬件并不是按照这里的方式来存储page table。假设VPN是27位，PPN是44位，你可以认为page table是从0到2^27，但是实际上并不是这样。实际中，page table是一个多级的结构。下图是一个真正的RISC-V page table结构和硬件实现。
![[Pasted image 20251220152559.png]]


每个进程都有独属于自己的Page table。
![[Pasted image 20251220153917.png]]


TLB：一种特殊的cache
普通cache缓存数据/指令（I\D cache）
TLB缓存：VPN -> PPN

TLB 类似一个小型缓存，通常为全相联。题目如果给 TLB 项目数 (如 TLB有64项) 则**索引位=0，全部并行比较Tag**。若是多路组相联TLB，则 Index = log2(组数)。TLB 的 Tag 通常对应VPN高位。还有TLB项内容位数：= VPN Tag位 + PPN位 + 有效位/保护位等管理位。例如：若虚拟地址20位VPN、物理地址18位PPN，加上有效位，则每项应至少存储 VPN的Tag(可能减去索引位)、PPN和1位有效，共计位数。

##### 常见考点包括：**页表大小计算**、**多级页表的结构**、**地址转换过程**。
典型题：给定虚拟地址位数、物理内存大小、页大小和每个页表项大小，计算单级页表总大小；若采用两级页表，求最小页表总开销以及一级、二级页号位数。也有题目要求根据给定的页表或 TLB 内容对虚拟地址进行手工转换，得到物理地址。例如，会给出某进程的页表或TLB项，让你把某个虚拟地址拆分为VPN和页内偏移，然后查表得到物理帧号合成物理地址。

![[Pasted image 20251220160044.png]]
>$100*2^{20}*4$

![[Pasted image 20251220160108.png]]
>$log_2{4096/4}=10$->1st level bits。20-10->2nd level bits。
>（4096+4096）*100

#### cache
- **址位划分**：给容量/块大小/组数/路数 → 求 Tag/Index/Offset；或反过来。
- **访问序列命中/失效**：直映 / 组相联 + LRU/MRU，逐次判断 hit/miss。
- **写策略**：write-through / write-back，write-allocate / no-allocate 的搭配概念题。
- **I-cache & D-cache 分离**：把同一段程序的取指地址和访存地址分别落到两套 cache 上模拟。

为什么需要cache：CPU 越来越快，但内存（RAM）慢得多
三条需要注意的地方：
- Cache 里的东西，永远是“复制品”。> **Cache 永远只是内存的一个“副本（copy）”**真正的数据“老家”在内存（RAM） Cache 只是抄一份放近一点所以：  **程序行为不会因为有没有 Cache 而改变，只是快慢不同**
- Cache 利用的是“局部性原理”。
	时间局部性（Temporal Locality） **刚用过的东西，很可能马上还会用**。如：for 循环里的变量，刚访问过的数组元素
	空间局部性（Spatial Locality）意思是：**用一个数据，常常会顺手用它旁边的数据**数组 `a[0]`、`a[1]`、`a[2]`顺序读内存。
- Cache 是自动的，程序员不用管**不需要为 Cache 改一行代码**Cache 的事情：全部由 **硬件（cache controller）** 自动完成。

对于一个地址，会被分为，| Tag | Index | Offset |
Index：去 Cache 的哪一行。Tag：判断这行是不是我要的。Offset：这一行里具体哪个字节

##### **地址映射计算**：
**Index数 = (缓存总行数/组相联度)**，**Offset位数 = log<sub>2</sub>(块大小)**，**Index位数 = log<sub>2</sub>(组数)**，**Tag位数 = 地址总位数 – Index位数 – Offset位数**。解题时先换算单位：注意区分字节(Byte)和字(Word)，如果题目以字为单位，要乘以字长转换为字节。然后逐步求出块数、组数、Index和Offset位。

![[Pasted image 20251220162003.png]]
Offset =$log_2{64}=6$ entry = $4KiB/64B = 64$ 
Index Four-way set associative = $log_2{entry/4}=4$
Index Fully associative = $log_2{entry/entry} = 0$
Tag = $log_2{4GiB} - Index - Offset$

解题时先换算单位：注意区分字节(Byte)和字(Word)，如果题目以字为单位，要乘以字长转换为字节。然后逐步求出块数、组数、Index和Offset位。


miss类型：
- 先想象一个“无限大 + 全相联”的 cache：那里面发生的 miss 都只能 **compulsory miss**（第一次见到必然 miss）。
- 再把 cache 变成“有限大但仍全相联”：新增的 miss 就归为 **capacity miss**（装不下造成的）。
- 再把“全相联”削弱成“没那么相联”（比如到 direct-mapped 或中间形态）：剩下新增的 miss 就是 **conflict miss**。


- **Compulsory miss：**程序第一次访问数组 `a[0]` 所在的 block → 必 miss（冷启动）。
- **Capacity miss：**你顺序扫描一个很大的数组，数组的活跃数据总量明显 > cache 容量 → 你早先装进来的块不断被挤掉，即使 fully associative 也会 miss。
- **Conflict miss：**direct-mapped 下，你反复交替访问两个地址 A 和 B，它俩刚好映射到同一个 cache 行（同一个 index）→ A 把 B 踢掉，B 又把 A 踢掉，“乒乓”式 miss；但如果改成 fully associative（同容量）就可能不 miss。

L1注重速度（命中时间），L2/L3更注重降低缺失率和主存带来的大延迟。

##### **命中率/AMAT计算**
命中率 = 命中次数/总访问次数。手工模拟序列可直接数出命中次数。若题目给命中率和时间开销，让算平均内存访问时间AMAT，则用公式逐级代入。例如：若命中率95%，L1访问1ns，主存10ns，则AMAT = 1ns + 0.05×(10ns) = 1.5ns（取决于是全局缺失率还是局部缺失率要小心）。多级缓存情况要看题意：常见公式 AMAT = L1时间 + 缺失率<sub>L1</sub>×(L2时间 + 缺失率<sub>L2</sub>×(… + 主存惩罚))。仔细区分**局部缺失率**（某级未命中占该级访问的比例）和**全局缺失率**（相对于最初总访问）。

##### 写策略：
写直达每次更新数据同时写主存，因此需要总线带宽高，但实现简单且无需Dirty位；写回则在缓存中暂存修改，替换出时才写回主存，效率高但要维护Dirty位。考试常以**判断题**形式考查，如“没有Dirty位则缓存一定采用写直达”是 True。也考查写分配(Write Allocate) vs 非写分配(No-write allocate)：前者在写Miss时将块调入缓存，后者则直接写主存不调入。这些可通过记忆口诀区分：“**写直达+无分配**常配对用于简单缓存，**写回+写分配**常配对用于性能优化”。

##### **缓存模拟**：
画出表格列包含：地址、拆分出的 Tag/Index/Offset、访问结果（Hit/Miss），以及缓存行状态。对于直映缓存，可用一维表模拟；组相联则用二维表表示各组内各路。按照地址序列逐个更新表格：未命中时，将数据放入相应组的某路（若满按照LRU替换）。

![[Pasted image 20251220164137.png]]

#### Pipeline
题型涵盖选择题、判断题和综合计算题等。常见选择题会考查流水线原理（如“流水线提高的是吞吐率而非单条指令延迟”）、数据冒险类型判定、转发的作用等。判断题往往涉及流水线特点（如“流水线各阶段应在一个周期内完成”| False）。综合题则经常给出一段指令序列或代码片段，要求分析流水线执行过程：例如绘制时空图、标出各指令在各周期的阶段，插入气泡（NOP）的位置，或计算总周期数/CPI 等。有时还结合 RISC-V/MIPS 汇编代码，要求指出数据相关和控制相关，并确定有无转发时需要插入的暂停周期数。另一类综合题会给出不同指令占比和延迟，要求计算平均 CPI或比较不同处理器性能。

- R-Format。像 `add x1, x2, x3` 这种 **“寄存器 + 寄存器”** 的指令，
- I-Format。像 `addi x1, x2, 10` 这种 **“寄存器 + 常数”** 的指令。
- Load 指令也是 I-Format。load 需要：一个寄存器（基地址）一个立即数（offset）一个目的寄存器（装数据）。
- S-Format。为什么 `sw` 不能用 I-format？- store **没有 rd**它需要：一个寄存器放地址，一个寄存器放要写的数据，一个 offset
- B-Format。指令必须对齐，**branch offset 以 2 字节为单位**最低位永远是 0，不存。
- U-Format
- J-Format

结构冒险：
- 取指令要用内存
- 存数据也要用内存
- 结果：**两个指令同时要用同一个东西**
解决方法：
- **加硬件**，如：指令内存和数据内存分开（cache）

数据冒险：
```
add x1, x2, x3   # 结果要写到 x1
sub x4, x1, x5   # 马上用 x1
```
解决方法：
停一下（stall）
- 插入一个“什么都不干的指令（nop）”
- 等结果出来再继续

转发（forwarding）
- 不等它写回寄存器
- **直接把 ALU 算出来的结果“抄近路”送过去**
![[Pasted image 20251220170922.png]]
无转发时，相邻第一条指令nop两个周期，相邻第二条指令nop三个周期。

**但是**有一种数据冒险没法救
```
lw x1, 0(x2)
add x3, x1, x4
```
- `lw` 的数据 **要到“内存阶段”才出来**，太晚了，**来不及转发**
**必须停 1 个周期**

控制冒险：
```
beq x1, x2, label
add x3, x4, x5   # 可能不该执行
```
- 分支结果要到后面才知道
- 但后面的指令已经开始执行了

解决方法：
1. **猜**（branch prediction）
2. 猜错了就：
    - 把已经执行的指令当成 **nop**
    - 丢掉重来

猜错了，当前分支指令后的全部指令变成nop
![[Pasted image 20251220171115.png]]


![[Pasted image 20251220172122.png]]


![[Pasted image 20251220172148.png]]![[Pasted image 20251220172220.png]]


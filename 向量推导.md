> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [yuanbao.tencent.com](https://yuanbao.tencent.com/chat/naQivTmsDa/24889ed7-df8f-4843-9d58-adf6ce18f495)

> 腾讯元宝是基于腾讯混元大模型的 AI 应用，可以帮你写作绘画文案翻译编程搜索阅读总结的全能助手

[[机器学习#^nl02quuo9if]]
给定的函数是：

```
Ew^​=(y−Xw^)T(y−Xw^)
```

其中，y 是 m×1 的标签向量，X 是 m×n 的特征矩阵（也称为设计矩阵），w^ 是 n×1 的待求参数向量。

我们的目标是求梯度：

```
∂w^∂Ew^​​=∂w^∂​[(y−Xw^)T(y−Xw^)]
```

### 推导过程

**第一步：展开目标函数**
<!--ID: 1764664440006-->


为了求导方便，我们先将函数展开。令 r=y−Xw^，这个向量 r 表示残差（预测值与真实值的差）。那么原函数变为：

```
Ew^​=rTr=∥r∥22​
```

这实际上是残差向量 r 的 L2 范数的平方。

将 r=y−Xw^ 代回并展开：

```
Ew^​​=(y−Xw^)T(y−Xw^)=(yT−w^TXT)(y−Xw^)=yTy−yTXw^−w^TXTy+w^TXTXw^​(运用了 (AB)T=BTAT 的性质)​
```

**第二步：简化表达式**

注意到中间的两项 −yTXw^ 和 −w^TXTy 其实是相等的，因为它们都是标量（1x1 的矩阵），且一个标量的转置等于其自身：

```
yTXw^=(yTXw^)T=w^TXTy
```

因此，−yTXw^−w^TXTy=−2w^TXTy。

于是，目标函数简化为：

```
Ew^​=yTy−2w^TXTy+w^TXTXw^
```

**第三步：对 w^ 求偏导**

现在我们对这个标量函数 Ew^​关于向量 w^ 求梯度。我们需要使用一些向量求导的法则（类比标量求导）：

1.  1.1.
    
    ∂w^∂(w^Ta)​=a（其中 a 是与 w^ 无关的向量）
    
2.  2.2.
    
    ∂w^∂(w^TAw^)​=(A+AT)w^（其中 A 是与 w^ 无关的矩阵）
    

让我们逐项求导：

1.  1.1.
    
    常数项 yTy 的导数为 0。
    
2.  2.2.
    
    线性项 −2w^T(XTy) 的导数：根据法则 1，令 a=XTy，此项导数为 −2(XTy)。
    
3.  3.3.
    
    二次项 w^T(XTX)w^ 的导数：根据法则 2，令 A=XTX。因为 AT=(XTX)T=XTX=A，所以 A 是对称矩阵。因此此项导数为 (A+AT)w^=2Aw^=2(XTX)w^。
    

**第四步：合并结果**

将各项的导数相加：

```
∂w^∂Ew^​​​=0+(−2XTy)+2(XTX)w^=2(XTX)w^−2XTy​
```

提取公因式 2 和 XT：

```
∂w^∂Ew^​​=2XT(Xw^−y)
```

**最终结果：**

```
∂w^∂Ew^​​=2XT(Xw^−y)
```

这与您图片中的公式完全一致。

### 总结与直观理解

*   ••
    
    **核心思想**：这个推导过程本质上是将向量形式的平方损失函数展开，并利用向量微积分的规则进行求导。
    
*   ••
    
    **关键步骤**：认识到 XTX 是一个对称矩阵，这简化了二次项的求导。
    
*   ••
    
    **几何意义**：梯度 ∂w^∂E​指向函数值增长最快的方向。在机器学习中，我们通常令梯度为零 \frac{\partial E}{\partial \hat{w}}} = 0 来求解最小化损失的参数 w^，即得到经典的正规方程（Normal Equation）：
    
    ```
    XTXw^=XTy
    ```
    
<!--ID: 1764664440010-->


希望这个详细的逐步推导能帮助您完全理解这个过程！
---
annotation-target: "[[机器学习(周志华).pdf]]"
---


>%%
>```annotation-json
>{"created":"2025-11-04T08:32:34.596Z","updated":"2025-11-04T08:32:34.596Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":52222,"end":52271},{"type":"TextQuoteSelector","exact":"查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往偏低;而查全率高时，查准率往往偏低","prefix":" (2.8) . -N '-F n一+一P-T R (2.9) ","suffix":".例如，若希望将好瓜尽可能多地选出来，则可通过增加选瓜的数量来实"}]}]}
>```
>%%
>*%%PREFIX%%(2.8) . -N '-F n一+一P-T R (2.9)%%HIGHLIGHT%% ==查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往偏低;而查全率高时，查准率往往偏低== %%POSTFIX%%.例如，若希望将好瓜尽可能多地选出来，则可通过增加选瓜的数量来实*
>%%LINK%%[[#^7oai99q7zru|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7oai99q7zru


>%%
>```annotation-json
>{"created":"2025-11-04T08:34:26.300Z","updated":"2025-11-04T08:34:26.300Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":52816,"end":52890},{"type":"TextQuoteSelector","exact":"习器在样本总体上的查全率、 查准率 .在进行比较时?若一个学习器的 P-R 曲线被另一个学习器的曲线完全\"包住 \" ， 则可断言后者的性能优于前者，","prefix":"率图 2 .3 P-R曲线与平衡点示意图P-R 图直观地显示出学","suffix":" 例如图 2 .3 中学习器 A 的性能优于学习器 C; 如果两"}]}]}
>```
>%%
>*%%PREFIX%%率图 2 .3 P-R曲线与平衡点示意图P-R 图直观地显示出学%%HIGHLIGHT%% ==习器在样本总体上的查全率、 查准率 .在进行比较时?若一个学习器的 P-R 曲线被另一个学习器的曲线完全"包住 " ， 则可断言后者的性能优于前者，== %%POSTFIX%%例如图 2 .3 中学习器 A 的性能优于学习器 C; 如果两*
>%%LINK%%[[#^ur1j4kdcrq8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ur1j4kdcrq8


>%%
>```annotation-json
>{"created":"2025-11-04T08:51:18.507Z","updated":"2025-11-04T08:51:18.507Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":55972,"end":56116},{"type":"TextQuoteSelector","exact":"进行学习器的比较时， 与 P-R 图相似， 若一个学习器的 ROC 曲线被另一个学习器的曲线完全\"包住\"， 则可断言后者的性能优于前者;若两个学习器的 ROC 曲线发生交叉，则难以-般性地断言两者孰优孰劣 . 此时如果一定要进行比较， 则较为合理的判据是比较 ROC 曲线下的面积，即 A","prefix":"记点的坐标为 (X +去 ， ν)，然后用线段连接相邻点即得 .","suffix":"UC (Area Under ROC Curve)，如图 2.4"}]}]}
>```
>%%
>*%%PREFIX%%记点的坐标为 (X +去 ， ν)，然后用线段连接相邻点即得 .%%HIGHLIGHT%% ==进行学习器的比较时， 与 P-R 图相似， 若一个学习器的 ROC 曲线被另一个学习器的曲线完全"包住"， 则可断言后者的性能优于前者;若两个学习器的 ROC 曲线发生交叉，则难以-般性地断言两者孰优孰劣 . 此时如果一定要进行比较， 则较为合理的判据是比较 ROC 曲线下的面积，即 A== %%POSTFIX%%UC (Area Under ROC Curve)，如图 2.4*
>%%LINK%%[[#^45bfz0i9f9b|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^45bfz0i9f9b


>%%
>```annotation-json
>{"created":"2025-11-04T09:04:39.852Z","updated":"2025-11-04T09:04:39.852Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":58021,"end":58265},{"type":"TextQuoteSelector","exact":" FPR是式 (2.19)定义的假E例率， FNR == 1 - TPR 是假反例率.代价曲线的绘制很简单: ROC 由线上每...点对应了代价平面上的二条线段 7 设 ROC 曲线上点的坐标为 (TPR ， FPR)，则可相应计算出 FNR，然后在代价平面上绘制一条从 (O ， FPR) 到 (l ， FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价;如此将 ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的自积即为在所有条件下学习器的期望总","prefix":"  - p)  xωst10' (2.24) (2.25) 其中","suffix":"体代价，如图 2.5 所示.2.4 比较检验更多关于假设检验的介"}]}]}
>```
>%%
>*%%PREFIX%%- p)  xωst10' (2.24) (2.25) 其中%%HIGHLIGHT%% ==FPR是式 (2.19)定义的假E例率， FNR == 1 - TPR 是假反例率.代价曲线的绘制很简单: ROC 由线上每...点对应了代价平面上的二条线段 7 设 ROC 曲线上点的坐标为 (TPR ， FPR)，则可相应计算出 FNR，然后在代价平面上绘制一条从 (O ， FPR) 到 (l ， FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价;如此将 ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的自积即为在所有条件下学习器的期望总== %%POSTFIX%%体代价，如图 2.5 所示.2.4 比较检验更多关于假设检验的介*
>%%LINK%%[[#^g704e1kiq07|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^g704e1kiq07


>%%
>```annotation-json
>{"created":"2025-11-04T10:18:20.216Z","updated":"2025-11-04T10:18:20.216Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":58468,"end":58688},{"type":"TextQuoteSelector","exact":"机器学习中性能比较这件事要比大家想象的复杂得多.这里面涉及几个重要因素:首先，我们希望比较的是泛化性能，然而通过实验评估方法我们获得的是测试集上的性能，两者的对比结果可能未必相同;第二，测试集上的性能与测试集本身的选择有很大关系，且不论使用不同大小的测试集会得到不同的结果，即使用相同大小的测试集?若包含的测试样例不同，测试结果也会有不同;第二，很多机器学习算法本身有一定的随机性，即便用相同的参数设置在同一个测试集上多次运行，其结果也会有","prefix":"这个\"比较\"呢?是直接取得性能度量的值然后\"比大小\"吗?实际上，","suffix":"不同.那么，有没有适当的方法对学习器的性能进行比较呢?统计假设检"}]}]}
>```
>%%
>*%%PREFIX%%这个"比较"呢?是直接取得性能度量的值然后"比大小"吗?实际上，%%HIGHLIGHT%% ==机器学习中性能比较这件事要比大家想象的复杂得多.这里面涉及几个重要因素:首先，我们希望比较的是泛化性能，然而通过实验评估方法我们获得的是测试集上的性能，两者的对比结果可能未必相同;第二，测试集上的性能与测试集本身的选择有很大关系，且不论使用不同大小的测试集会得到不同的结果，即使用相同大小的测试集?若包含的测试样例不同，测试结果也会有不同;第二，很多机器学习算法本身有一定的随机性，即便用相同的参数设置在同一个测试集上多次运行，其结果也会有== %%POSTFIX%%不同.那么，有没有适当的方法对学习器的性能进行比较呢?统计假设检*
>%%LINK%%[[#^773vm94jfse|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^773vm94jfse


>%%
>```annotation-json
>{"created":"2025-11-04T10:18:43.193Z","updated":"2025-11-04T10:18:43.193Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":58715,"end":58828},{"type":"TextQuoteSelector","exact":"统计假设检验(hypothesis test)为我们进行学习器 t性能比较提供了重要依据.基于假设检验结果我们可推断出，若在测试集上观察到学习器 A 比 B 好，则 A 的泛化性能是否在统计意义上优于 B，以及这个结论的把握有","prefix":"结果也会有不同.那么，有没有适当的方法对学习器的性能进行比较呢?","suffix":"多大.下面我们先介绍两种最基本的假设检验，然后介绍几种常用的机器"}]}]}
>```
>%%
>*%%PREFIX%%结果也会有不同.那么，有没有适当的方法对学习器的性能进行比较呢?%%HIGHLIGHT%% ==统计假设检验(hypothesis test)为我们进行学习器 t性能比较提供了重要依据.基于假设检验结果我们可推断出，若在测试集上观察到学习器 A 比 B 好，则 A 的泛化性能是否在统计意义上优于 B，以及这个结论的把握有== %%POSTFIX%%多大.下面我们先介绍两种最基本的假设检验，然后介绍几种常用的机器*
>%%LINK%%[[#^rjq7jfxkfc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rjq7jfxkfc


>%%
>```annotation-json
>{"created":"2025-11-04T10:24:35.217Z","updated":"2025-11-04T10:24:35.217Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":61452,"end":61487},{"type":"TextQuoteSelector","exact":"欲进行有效的假设检验，一个重要前提是测试错误率均为泛化错误率的独立采样","prefix":"是自由度为 k-1 的 t分布上尾部累积分布为 α/2的临界值.","suffix":".然而，通常情况下由于样本有限，在使用交叉验证等实验估计方法时，"}]}]}
>```
>%%
>*%%PREFIX%%是自由度为 k-1 的 t分布上尾部累积分布为 α/2的临界值.%%HIGHLIGHT%% ==欲进行有效的假设检验，一个重要前提是测试错误率均为泛化错误率的独立采样== %%POSTFIX%%.然而，通常情况下由于样本有限，在使用交叉验证等实验估计方法时，*
>%%LINK%%[[#^5tlucw8hgvr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5tlucw8hgvr


>%%
>```annotation-json
>{"created":"2025-11-04T10:31:50.979Z","updated":"2025-11-04T10:31:50.979Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":65462,"end":65559},{"type":"TextQuoteSelector","exact":"对学习算法除了通过实验估计其泛化性能?人们往往还希望了解它\"为什么\"具有这样的性能\"偏差方差分解\" (bias-variance  decomposition)是解释学习算法泛化性能的一种重要工","prefix":"0 3.0 图 2.8 Friedman检验图2.5 偏差与方差","suffix":"具.偏差方差分解试图对学习算法的期望泛化错误率进行拆解.我们知道"}]}]}
>```
>%%
>*%%PREFIX%%0 3.0 图 2.8 Friedman检验图2.5 偏差与方差%%HIGHLIGHT%% ==对学习算法除了通过实验估计其泛化性能?人们往往还希望了解它"为什么"具有这样的性能"偏差方差分解" (bias-variance  decomposition)是解释学习算法泛化性能的一种重要工== %%POSTFIX%%具.偏差方差分解试图对学习算法的期望泛化错误率进行拆解.我们知道*
>%%LINK%%[[#^317gtwt80uj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^317gtwt80uj


>%%
>```annotation-json
>{"created":"2025-11-04T10:35:24.325Z","updated":"2025-11-04T10:35:24.325Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":66524,"end":66699},{"type":"TextQuoteSelector","exact":"真实结果的偏离程度，即刻画了学习算法本身的拟合能力;方差 (2.38)度量了闰样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响;噪声 (2.39) 则表达了在当前任务上任何学习算法所能达到的期望泛化误差的F界，即刻画了学习问题本身的难度.偏差一方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同","prefix":"训练轮数，集成学习方法可控制基学习器个数第 2 章模型评估与选择","suffix":"决定的.给定学习任务?为了取得好的泛化性能，则需使偏差较小，即能"}]}]}
>```
>%%
>*%%PREFIX%%训练轮数，集成学习方法可控制基学习器个数第 2 章模型评估与选择%%HIGHLIGHT%% ==真实结果的偏离程度，即刻画了学习算法本身的拟合能力;方差 (2.38)度量了闰样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响;噪声 (2.39) 则表达了在当前任务上任何学习算法所能达到的期望泛化误差的F界，即刻画了学习问题本身的难度.偏差一方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同== %%POSTFIX%%决定的.给定学习任务?为了取得好的泛化性能，则需使偏差较小，即能*
>%%LINK%%[[#^efdjnoc7mp4|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^efdjnoc7mp4


>%%
>```annotation-json
>{"created":"2025-11-07T06:48:41.152Z","updated":"2025-11-07T06:48:41.152Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":73083,"end":73120},{"type":"TextQuoteSelector","exact":"若属性值间存在\"序\" (order)关系，可通过连续化将其转化为连续值，例","prefix":" =  {(Xi ， Yi)}立l' 其中玛巴lR.对离散属性，","suffix":"如二线性模型若将元序属性 i主续化，则会不恰当地引入序关系，对后"}]}]}
>```
>%%
>*%%PREFIX%%=  {(Xi ， Yi)}立l' 其中玛巴lR.对离散属性，%%HIGHLIGHT%% ==若属性值间存在"序" (order)关系，可通过连续化将其转化为连续值，例== %%POSTFIX%%如二线性模型若将元序属性 i主续化，则会不恰当地引入序关系，对后*
>%%LINK%%[[#^5vroks9ci1a|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5vroks9ci1a


>%%
>```annotation-json
>{"created":"2025-11-07T06:48:53.864Z","updated":"2025-11-07T06:48:53.864Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":73252,"end":73287},{"type":"TextQuoteSelector","exact":" 若属性值间不存在序关系，假定有 k 个属性值，则通常转化为 k 维向","prefix":"值\"高\" \"中\" \"低\"可转化为 {1札 0.5 ， 0.0};","suffix":"量，例如属性\"瓜类\"的取值\"西瓜\" \"南瓜\" \"黄瓜\"可转化为 "}]}]}
>```
>%%
>*%%PREFIX%%值"高" "中" "低"可转化为 {1札 0.5 ， 0.0};%%HIGHLIGHT%% ==若属性值间不存在序关系，假定有 k 个属性值，则通常转化为 k 维向== %%POSTFIX%%量，例如属性"瓜类"的取值"西瓜" "南瓜" "黄瓜"可转化为*
>%%LINK%%[[#^ws89233fr9n|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ws89233fr9n


>%%
>```annotation-json
>{"created":"2025-11-07T06:56:20.918Z","updated":"2025-11-07T06:56:20.918Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":75299,"end":75378},{"type":"TextQuoteSelector","exact":"此时可解出多个仙 ， 它们都能使均方误差最小化.选择哪一个解作为输出 ，将由学习算法的归纳偏好决定， 常见的做法是引入正则化 (regularization)","prefix":"甚至超过样例数，导致 X 的列数多于行数 ， xTx显然不满秩.","suffix":"项.线性模型虽简单，却有丰富的变化. 例如对于样例忡 ， y) "}]}]}
>```
>%%
>*%%PREFIX%%甚至超过样例数，导致 X 的列数多于行数 ， xTx显然不满秩.%%HIGHLIGHT%% ==此时可解出多个仙 ， 它们都能使均方误差最小化.选择哪一个解作为输出 ，将由学习算法的归纳偏好决定， 常见的做法是引入正则化 (regularization)== %%POSTFIX%%项.线性模型虽简单，却有丰富的变化. 例如对于样例忡 ， y)*
>%%LINK%%[[#^uoe65796of|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^uoe65796of


>%%
>```annotation-json
>{"created":"2025-11-13T10:02:12.913Z","updated":"2025-11-13T10:02:12.913Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":74788,"end":74828},{"type":"TextQuoteSelector","exact":"令 Ew = (y - X 'ÛJ )T (y - X'ÛJ)，对'ÛJ求导得到","prefix":"gmin (ν- X 'ÛJ )T (y - X 'ÛJ) 咀3","suffix":"åB生中万左= 2X1 (Xψ - y) (3.9) (3.10"}]}]}
>```
>%%
>*%%PREFIX%%gmin (ν- X 'ÛJ )T (y - X 'ÛJ) 咀3%%HIGHLIGHT%% ==令 Ew = (y - X 'ÛJ )T (y - X'ÛJ)，对'ÛJ求导得到== %%POSTFIX%%åB生中万左= 2X1 (Xψ - y) (3.9) (3.10*
>%%LINK%%[[#^nl02quuo9if|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nl02quuo9if


>%%
>```annotation-json
>{"created":"2025-11-13T10:03:07.146Z","updated":"2025-11-13T10:03:07.146Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":74919,"end":74997},{"type":"TextQuoteSelector","exact":"当 XTX 为满秩矩阵 (full-rank matrix)或正走矩阵 (positive definite  ma-trix)时，令式 (3.10)为零可","prefix":"矩阵逆的计算，比单变量情形要复杂一些.下面我们做一个简单的讨论.","suffix":"得旷= (XTX)~1XTy ，  (3.11) 其中 (XTX"}]}]}
>```
>%%
>*%%PREFIX%%矩阵逆的计算，比单变量情形要复杂一些.下面我们做一个简单的讨论.%%HIGHLIGHT%% ==当 XTX 为满秩矩阵 (full-rank matrix)或正走矩阵 (positive definite  ma-trix)时，令式 (3.10)为零可== %%POSTFIX%%得旷= (XTX)~1XTy ，  (3.11) 其中 (XTX*
>%%LINK%%[[#^0vnhrqj9l0a|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0vnhrqj9l0a


>%%
>```annotation-json
>{"created":"2025-11-13T10:04:48.315Z","updated":"2025-11-13T10:04:48.315Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":75818,"end":75830},{"type":"TextQuoteSelector","exact":"单调可微函数 g(.) ","prefix":"3 对数儿率回归 57 g(.) 连续且充分光滑 更一般地，考虑","suffix":"， 令y  = g-l(WTX 十 的， (3. 15 ) 这样"}]}]}
>```
>%%
>*%%PREFIX%%3 对数儿率回归 57 g(.) 连续且充分光滑 更一般地，考虑%%HIGHLIGHT%% ==单调可微函数 g(.)== %%POSTFIX%%， 令y  = g-l(WTX 十 的， (3. 15 ) 这样*
>%%LINK%%[[#^jfpn0ubdkrj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jfpn0ubdkrj


>%%
>```annotation-json
>{"created":"2025-11-13T10:05:18.095Z","updated":"2025-11-13T10:05:18.095Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":76061,"end":76134},{"type":"TextQuoteSelector","exact":"的是分类任务该怎么办?答案蕴涵在式 (3.15)的广义线性模型中.只需找一个单调可做函数将分类任务的真实标记 υ与线性回归模型的预测值联系起来 .","prefix":" 对数几率回归上一节讨论了如何使用线性模型进行回归学习，但若要做","suffix":"考虑二分类任务， 其输出标记 νε{0 ， 1}，而线性回归模型"}]}]}
>```
>%%
>*%%PREFIX%%对数几率回归上一节讨论了如何使用线性模型进行回归学习，但若要做%%HIGHLIGHT%% ==的是分类任务该怎么办?答案蕴涵在式 (3.15)的广义线性模型中.只需找一个单调可做函数将分类任务的真实标记 υ与线性回归模型的预测值联系起来 .== %%POSTFIX%%考虑二分类任务， 其输出标记 νε{0 ， 1}，而线性回归模型*
>%%LINK%%[[#^ifhngdbvul|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ifhngdbvul


>%%
>```annotation-json
>{"created":"2025-11-13T10:06:45.073Z","updated":"2025-11-13T10:06:45.073Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":77099,"end":77154},{"type":"TextQuoteSelector","exact":"式 (3.18)实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为\"对数几率回","prefix":"亦称 logit)l丑一旦一-1-y (3.21) 由此可看出，","suffix":"归\" (logistic regression，亦称 logit"}]}]}
>```
>%%
>*%%PREFIX%%亦称 logit)l丑一旦一-1-y (3.21) 由此可看出，%%HIGHLIGHT%% ==式 (3.18)实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为"对数几率回== %%POSTFIX%%归" (logistic regression，亦称 logit*
>%%LINK%%[[#^1u9z2ypryke|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1u9z2ypryke


>%%
>```annotation-json
>{"created":"2025-11-13T10:10:01.962Z","updated":"2025-11-13T10:10:01.962Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":77449,"end":77486},{"type":"TextQuoteSelector","exact":"将式 (3.18)中的 u 视为类后验概率估计 p(y =  1 1 x)","prefix":"9 F面我们来看看如何确定式 (3.18)中的 ω 和 b. 若","suffix":" ， 则式 (3.19)可重写为显然有p(y =  11  x)"}]}]}
>```
>%%
>*%%PREFIX%%9 F面我们来看看如何确定式 (3.18)中的 ω 和 b. 若%%HIGHLIGHT%% ==将式 (3.18)中的 u 视为类后验概率估计 p(y =  1 1 x)== %%POSTFIX%%， 则式 (3.19)可重写为显然有p(y =  11  x)*
>%%LINK%%[[#^1rqyooo7pm7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1rqyooo7pm7


>%%
>```annotation-json
>{"created":"2025-11-13T10:11:18.599Z","updated":"2025-11-13T10:11:18.599Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":78066,"end":78198},{"type":"TextQuoteSelector","exact":"式 (3.27)是关于 β 的高阶可导连续凸函数，根据凸优化理论 [Boyd and Vandenberghe,  2004]，经典的数值优化算法如梯度下降法 (gradient descent method)、牛顿法(Newton method)等都可求得其最优","prefix":"β) =  E (-YißT如叫l+e内)) . (3.27) ","suffix":"解，于是就得到β* = argmine(β) . β 以牛顿法为"}]}]}
>```
>%%
>*%%PREFIX%%β) =  E (-YißT如叫l+e内)) . (3.27)%%HIGHLIGHT%% ==式 (3.27)是关于 β 的高阶可导连续凸函数，根据凸优化理论 [Boyd and Vandenberghe,  2004]，经典的数值优化算法如梯度下降法 (gradient descent method)、牛顿法(Newton method)等都可求得其最优== %%POSTFIX%%解，于是就得到β* = argmine(β) . β 以牛顿法为*
>%%LINK%%[[#^5iyx9qob63a|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5iyx9qob63a


>%%
>```annotation-json
>{"created":"2025-11-13T10:12:38.861Z","updated":"2025-11-13T10:12:38.861Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":78626,"end":78739},{"type":"TextQuoteSelector","exact":"LDA 的，思想非常朴素: 给定训练样例集 7 设法将样例投影到一条直线上 ，使得同类样例的投影点尽可能接近、 异类样例的投影点尽可能远离;在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别","prefix":"析稍有不同，前者假设了各类样本的协方差 别分析\"矩阵相同且满秩.","suffix":". 图 3.3 给出了一个二维示意图.X l 图 3.3 LDA"}]}]}
>```
>%%
>*%%PREFIX%%析稍有不同，前者假设了各类样本的协方差 别分析"矩阵相同且满秩.%%HIGHLIGHT%% ==LDA 的，思想非常朴素: 给定训练样例集 7 设法将样例投影到一条直线上 ，使得同类样例的投影点尽可能接近、 异类样例的投影点尽可能远离;在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别== %%POSTFIX%%. 图 3.3 给出了一个二维示意图.X l 图 3.3 LDA*
>%%LINK%%[[#^8x4y1577fuu|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^8x4y1577fuu


>%%
>```annotation-json
>{"created":"2025-11-13T10:19:06.948Z","updated":"2025-11-13T10:19:06.948Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":80749,"end":80834},{"type":"TextQuoteSelector","exact":"若将W 视为一个投影矩阵，则多分类 LDA将样本投影到 N-1 维空间，N-1 通常远小子数据原有的属性数.于是，可通过这个投影来减小样本点的维数，且投影过程中使用了类别","prefix":"b 的 N 一 1 个最大广义特征值所对应的特征向量组成的矩阵.","suffix":"信息?因此LDA也常被视为一种经典的监督降维技术3.5 多分类学"}]}]}
>```
>%%
>*%%PREFIX%%b 的 N 一 1 个最大广义特征值所对应的特征向量组成的矩阵.%%HIGHLIGHT%% ==若将W 视为一个投影矩阵，则多分类 LDA将样本投影到 N-1 维空间，N-1 通常远小子数据原有的属性数.于是，可通过这个投影来减小样本点的维数，且投影过程中使用了类别== %%POSTFIX%%信息?因此LDA也常被视为一种经典的监督降维技术3.5 多分类学*
>%%LINK%%[[#^1hptj8wnvfr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1hptj8wnvfr


>%%
>```annotation-json
>{"created":"2025-11-13T10:20:01.392Z","updated":"2025-11-13T10:20:01.392Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":80968,"end":81106},{"type":"TextQuoteSelector","exact":" ， 多分类学习的基本思路是\"拆解法飞即将多分类任务拆为若干个二分类任务求解.具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果.这里的关键是如何对多分类任务进行拆分，以及如何对多个分类器进行集","prefix":".不夫一般性，考虑 N 个类别 C1 ， C2γ •• ， CN","suffix":"成.本节主要介绍拆分策略.最经典的拆分策略有三种. \"一对一\" "}]}]}
>```
>%%
>*%%PREFIX%%.不夫一般性，考虑 N 个类别 C1 ， C2γ •• ， CN%%HIGHLIGHT%% ==， 多分类学习的基本思路是"拆解法飞即将多分类任务拆为若干个二分类任务求解.具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果.这里的关键是如何对多分类任务进行拆分，以及如何对多个分类器进行集== %%POSTFIX%%成.本节主要介绍拆分策略.最经典的拆分策略有三种. "一对一"*
>%%LINK%%[[#^4kl0zs8d5sx|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^4kl0zs8d5sx


>%%
>```annotation-json
>{"created":"2025-11-13T10:20:11.203Z","updated":"2025-11-13T10:20:11.203Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":81130,"end":81197},{"type":"TextQuoteSelector","exact":". \"一对一\" (One  vs. One，简称 OvO) 、 \"一对其余\" (One  vs.  Rest，简称 OvR)和\"多对多","prefix":"多个分类器进行集成.本节主要介绍拆分策略.最经典的拆分策略有三种","suffix":"\" (Many  vs.  Ma町，简称 MvM).给定数据集 "}]}]}
>```
>%%
>*%%PREFIX%%多个分类器进行集成.本节主要介绍拆分策略.最经典的拆分策略有三种%%HIGHLIGHT%% ==. "一对一" (One  vs. One，简称 OvO) 、 "一对其余" (One  vs.  Rest，简称 OvR)和"多对多== %%POSTFIX%%" (Many  vs.  Ma町，简称 MvM).给定数据集*
>%%LINK%%[[#^7bitf72358|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7bitf72358


>%%
>```annotation-json
>{"created":"2025-11-13T10:22:01.700Z","updated":"2025-11-13T10:22:01.700Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":82043,"end":82093},{"type":"TextQuoteSelector","exact":"MvM 的正、反类构造必须有特殊的设计，不能随意选取.这里我们介绍一种最常用的 MvM技术\"纠错输出","prefix":"个其他类作为反类.显然， OvO 和OvR是 MvM 的特例. ","suffix":"码\" (Error Correcting Output Code"}]}]}
>```
>%%
>*%%PREFIX%%个其他类作为反类.显然， OvO 和OvR是 MvM 的特例.%%HIGHLIGHT%% ==MvM 的正、反类构造必须有特殊的设计，不能随意选取.这里我们介绍一种最常用的 MvM技术"纠错输出== %%POSTFIX%%码" (Error Correcting Output Code*
>%%LINK%%[[#^w1o5qw97ps|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^w1o5qw97ps


>%%
>```annotation-json
>{"created":"2025-11-13T10:26:21.747Z","updated":"2025-11-13T10:26:21.747Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":83626,"end":83701},{"type":"TextQuoteSelector","exact":"'一个理论纠错牲质很好、 {ê.导致的三分类问题较难的编码，与另一个理论纠错性质差→些、但导致的二分类问题较简单的编码，最终产生的模型性能孰强孰弱很难","prefix":"别u于集的区分难度往往不间即其导致的二分类问题的难度不同盹;于是","suffix":"说.3.6 类别不平衡问题前面介绍的分类学习方法都有→个共同的基"}]}]}
>```
>%%
>*%%PREFIX%%别u于集的区分难度往往不间即其导致的二分类问题的难度不同盹;于是%%HIGHLIGHT%% =='一个理论纠错牲质很好、 {ê.导致的三分类问题较难的编码，与另一个理论纠错性质差→些、但导致的二分类问题较简单的编码，最终产生的模型性能孰强孰弱很难== %%POSTFIX%%说.3.6 类别不平衡问题前面介绍的分类学习方法都有→个共同的基*
>%%LINK%%[[#^pkcm6g1tey|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^pkcm6g1tey


>%%
>```annotation-json
>{"created":"2025-11-13T10:26:41.640Z","updated":"2025-11-13T10:26:41.640Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":83795,"end":83889},{"type":"TextQuoteSelector","exact":".例如有 998 个反例，但正例只有 2 个，那么学习方法只需返回一个永远将新样本预测为反例的学习器，就能达到 99.8% 的精度;然而这样的学习器往往没有价值，因为它不能预测出任何正例.","prefix":"数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰","suffix":"类别不平衡 (cla胁imbalance)就是指分类任务中不同类"}]}]}
>```
>%%
>*%%PREFIX%%数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰%%HIGHLIGHT%% ==.例如有 998 个反例，但正例只有 2 个，那么学习方法只需返回一个永远将新样本预测为反例的学习器，就能达到 99.8% 的精度;然而这样的学习器往往没有价值，因为它不能预测出任何正例.== %%POSTFIX%%类别不平衡 (cla胁imbalance)就是指分类任务中不同类*
>%%LINK%%[[#^xte1mgauvul|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^xte1mgauvul


>%%
>```annotation-json
>{"created":"2025-11-13T10:28:26.249Z","updated":"2025-11-13T10:28:26.249Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":84549,"end":84590},{"type":"TextQuoteSelector","exact":"，需对其预测值进行调整，使其在基于式 (3.46)决策时，实际是在执行式 (3 .","prefix":") 67 但是，我们的分类器是基于式 (3.46)进行决策，因此","suffix":"47). 要做到这 点很容易，只需令y' y m 一---. -"}]}]}
>```
>%%
>*%%PREFIX%%) 67 但是，我们的分类器是基于式 (3.46)进行决策，因此%%HIGHLIGHT%% ==，需对其预测值进行调整，使其在基于式 (3.46)决策时，实际是在执行式 (3 .== %%POSTFIX%%47). 要做到这 点很容易，只需令y' y m 一---. -*
>%%LINK%%[[#^7p3l4u13tqg|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7p3l4u13tqg


>%%
>```annotation-json
>{"created":"2025-11-13T10:28:44.444Z","updated":"2025-11-13T10:28:44.444Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":83889,"end":83935},{"type":"TextQuoteSelector","exact":"类别不平衡 (cla胁imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情","prefix":"的精度;然而这样的学习器往往没有价值，因为它不能预测出任何正例.","suffix":"况.不失一般性，本节假定正类样例较少，反类样例较多.在现实的分类"}]}]}
>```
>%%
>*%%PREFIX%%的精度;然而这样的学习器往往没有价值，因为它不能预测出任何正例.%%HIGHLIGHT%% ==类别不平衡 (cla胁imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情== %%POSTFIX%%况.不失一般性，本节假定正类样例较少，反类样例较多.在现实的分类*
>%%LINK%%[[#^x8tw9vmstcq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^x8tw9vmstcq


>%%
>```annotation-json
>{"created":"2025-11-13T10:28:49.160Z","updated":"2025-11-13T10:28:49.160Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":84640,"end":84658},{"type":"TextQuoteSelector","exact":"类别不平衡学习的一个基本策略一\"再缩","prefix":"y' y m 一---. --1- y' 1- Y m+ 这就是","suffix":"放\" (rescaling). (3.48) 再缩放的思想虽简单"}]}]}
>```
>%%
>*%%PREFIX%%y' y m 一---. --1- y' 1- Y m+ 这就是%%HIGHLIGHT%% ==类别不平衡学习的一个基本策略一"再缩== %%POSTFIX%%放" (rescaling). (3.48) 再缩放的思想虽简单*
>%%LINK%%[[#^pz0z161i18|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^pz0z161i18


>%%
>```annotation-json
>{"created":"2025-11-13T10:29:10.971Z","updated":"2025-11-13T10:29:10.971Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":84691,"end":84733},{"type":"TextQuoteSelector","exact":"但实际操作却并不平凡，主要因为\"训练集是真实样本总体的无偏采样\"这个假设往往并不成立","prefix":"\" (rescaling). (3.48) 再缩放的思想虽简单，","suffix":"，也就是说 7 我们未必能有效地基于训练集观测几率来推断出真实几"}]}]}
>```
>%%
>*%%PREFIX%%" (rescaling). (3.48) 再缩放的思想虽简单，%%HIGHLIGHT%% ==但实际操作却并不平凡，主要因为"训练集是真实样本总体的无偏采样"这个假设往往并不成立== %%POSTFIX%%，也就是说 7 我们未必能有效地基于训练集观测几率来推断出真实几*
>%%LINK%%[[#^1ef5z3m5kyhh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1ef5z3m5kyhh


>%%
>```annotation-json
>{"created":"2025-11-13T10:29:40.825Z","updated":"2025-11-13T10:29:40.825Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":84965,"end":84973},{"type":"TextQuoteSelector","exact":"称为\"阔值移动\"","prefix":"训练好的分类器进行预测时，将式 (3.48)嵌入到其决策过程中，","suffix":" (threshold-moving). 欠采样法的时间开销通常"}]}]}
>```
>%%
>*%%PREFIX%%训练好的分类器进行预测时，将式 (3.48)嵌入到其决策过程中，%%HIGHLIGHT%% ==称为"阔值移动"== %%POSTFIX%%(threshold-moving). 欠采样法的时间开销通常*
>%%LINK%%[[#^pedk6apaj6d|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^pedk6apaj6d


>%%
>```annotation-json
>{"created":"2025-11-13T10:29:44.384Z","updated":"2025-11-13T10:29:44.384Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":84865,"end":84869},{"type":"TextQuoteSelector","exact":"过来样\"","prefix":"数日接近 7 然后再进行学习;第二类是对训练集里的正类样例进行\"","suffix":" (oversampling)，即增加一些正例使得正、反例数目接"}]}]}
>```
>%%
>*%%PREFIX%%数日接近 7 然后再进行学习;第二类是对训练集里的正类样例进行"%%HIGHLIGHT%% ==过来样"== %%POSTFIX%%(oversampling)，即增加一些正例使得正、反例数目接*
>%%LINK%%[[#^2eblppxwje|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2eblppxwje


>%%
>```annotation-json
>{"created":"2025-11-13T10:29:46.451Z","updated":"2025-11-13T10:29:46.451Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":84799,"end":84802},{"type":"TextQuoteSelector","exact":"欠采样","prefix":"现有技术大体上有三类做法:第一类是直接对训练集里的反类样例进行\"","suffix":"\" (undersampling)，即去除一些反倒使得正、反例数"}]}]}
>```
>%%
>*%%PREFIX%%现有技术大体上有三类做法:第一类是直接对训练集里的反类样例进行"%%HIGHLIGHT%% ==欠采样== %%POSTFIX%%" (undersampling)，即去除一些反倒使得正、反例数*
>%%LINK%%[[#^d0h46faxqvc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^d0h46faxqvc


>%%
>```annotation-json
>{"created":"2025-11-13T10:30:52.550Z","updated":"2025-11-13T10:30:52.550Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":85371,"end":85410},{"type":"TextQuoteSelector","exact":"代价敏感学习中将式 (3.48)中的 m-jm+ 用 cost+jcosr 代","prefix":"nsitive  le缸任代价敏感学习研究非 ing)的基础.在","suffix":"替即均等代价下的学习参见2.3.4节 可，其中 cost+ 是将"}]}]}
>```
>%%
>*%%PREFIX%%nsitive  le缸任代价敏感学习研究非 ing)的基础.在%%HIGHLIGHT%% ==代价敏感学习中将式 (3.48)中的 m-jm+ 用 cost+jcosr 代== %%POSTFIX%%替即均等代价下的学习参见2.3.4节 可，其中 cost+ 是将*
>%%LINK%%[[#^32ckdamn54g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^32ckdamn54g


>%%
>```annotation-json
>{"created":"2025-11-13T10:36:37.152Z","updated":"2025-11-13T10:36:37.152Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":72568,"end":72700},{"type":"TextQuoteSelector","exact":"线性模型形式简单、易于建模，但却蕴涵着机器学习中一些重要的基本思想.许多功能更为强大的非线性模型 (nonlinear model)可在线性模型的基础上通过引入层级结构或高维映射而得.此外，由于 ω 直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释'性","prefix":"ω2;'\" ;ωd)'  W 和 b 学得之后，模型就得以确定.","suffix":" (comprehensibility).例如若在西瓜问题中学得"}]}]}
>```
>%%
>*%%PREFIX%%ω2;'" ;ωd)'  W 和 b 学得之后，模型就得以确定.%%HIGHLIGHT%% ==线性模型形式简单、易于建模，但却蕴涵着机器学习中一些重要的基本思想.许多功能更为强大的非线性模型 (nonlinear model)可在线性模型的基础上通过引入层级结构或高维映射而得.此外，由于 ω 直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释'性== %%POSTFIX%%(comprehensibility).例如若在西瓜问题中学得*
>%%LINK%%[[#^y3ty7p4yph|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^y3ty7p4yph


>%%
>```annotation-json
>{"created":"2025-11-13T11:09:18.791Z","updated":"2025-11-13T11:09:18.791Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":92894,"end":92897},{"type":"TextQuoteSelector","exact":"信息铺","prefix":"结点的\"纯度\" (purity)越来越高.4.2.1 信息增益\"","suffix":"\" (information entropy)是度量样本集合纯度"}]}]}
>```
>%%
>*%%PREFIX%%结点的"纯度" (purity)越来越高.4.2.1 信息增益"%%HIGHLIGHT%% ==信息铺== %%POSTFIX%%" (information entropy)是度量样本集合纯度*
>%%LINK%%[[#^dnvnmnbg6ia|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^dnvnmnbg6ia


>%%
>```annotation-json
>{"created":"2025-11-13T11:12:57.014Z","updated":"2025-11-13T11:12:57.014Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":95769,"end":95793},{"type":"TextQuoteSelector","exact":"信息增益准则对可取值数目较多的属性有所偏好，为减","prefix":"样的决策树显然不具有泛化能力，无法对新样本进行有效预测.实际上，","suffix":"少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法 [Q"}]}]}
>```
>%%
>*%%PREFIX%%样的决策树显然不具有泛化能力，无法对新样本进行有效预测.实际上，%%HIGHLIGHT%% ==信息增益准则对可取值数目较多的属性有所偏好，为减== %%POSTFIX%%少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法 [Q*
>%%LINK%%[[#^gvsotgbu7y|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gvsotgbu7y


>%%
>```annotation-json
>{"created":"2025-11-13T11:17:59.946Z","updated":"2025-11-13T11:17:59.946Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":96794,"end":96829},{"type":"TextQuoteSelector","exact":"剪枝 (pruning)是决策树学习算法对付\"过拟合\"的主要手段.在决","prefix":"n GiniJndex(D ， α). αεA 4.3 剪枝处理","suffix":"策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，"}]}]}
>```
>%%
>*%%PREFIX%%n GiniJndex(D ， α). αεA 4.3 剪枝处理%%HIGHLIGHT%% ==剪枝 (pruning)是决策树学习算法对付"过拟合"的主要手段.在决== %%POSTFIX%%策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，*
>%%LINK%%[[#^f7yuo6yn9pc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^f7yuo6yn9pc


>%%
>```annotation-json
>{"created":"2025-11-13T11:18:34.584Z","updated":"2025-11-13T11:18:34.584Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":96964,"end":96987},{"type":"TextQuoteSelector","exact":"剪枝\" (prepruning)和\"后剪枝\"(","prefix":"过主动去掉一些分支来降低过拟合的风险.决策树剪枝的基本策略有\"预","suffix":"post\"pruning)  [Quinlan,  1993]."}]}]}
>```
>%%
>*%%PREFIX%%过主动去掉一些分支来降低过拟合的风险.决策树剪枝的基本策略有"预%%HIGHLIGHT%% ==剪枝" (prepruning)和"后剪枝"(== %%POSTFIX%%post"pruning)  [Quinlan,  1993].*
>%%LINK%%[[#^hxz2x8nxl3|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^hxz2x8nxl3


>%%
>```annotation-json
>{"created":"2025-11-13T11:20:37.409Z","updated":"2025-11-13T11:20:37.409Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":98974,"end":99076},{"type":"TextQuoteSelector","exact":"另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降?但在其基础上进行的后续划分却有可能导致性能显著提高;预剪枝基于\"贪心\"本质禁止这些分支展开 7 给预剪枝决策树带来了欠拟含的风","prefix":"可看出，于预页剪枝使得决策树的很多分支都没有\"展开试时间开销.但","suffix":"险4.3.2 后剪枝后剪枝先从训练集生成一棵完整决策树?例如基于"}]}]}
>```
>%%
>*%%PREFIX%%可看出，于预页剪枝使得决策树的很多分支都没有"展开试时间开销.但%%HIGHLIGHT%% ==另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降?但在其基础上进行的后续划分却有可能导致性能显著提高;预剪枝基于"贪心"本质禁止这些分支展开 7 给预剪枝决策树带来了欠拟含的风== %%POSTFIX%%险4.3.2 后剪枝后剪枝先从训练集生成一棵完整决策树?例如基于*
>%%LINK%%[[#^u6lyokut9t|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^u6lyokut9t


>%%
>```annotation-json
>{"created":"2025-11-13T11:26:21.854Z","updated":"2025-11-13T11:26:21.854Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":101313,"end":101358},{"type":"TextQuoteSelector","exact":"例如在父结点上使用了\"密度';;;0.381\" ，不会禁止在子结点上使用\"密度运0.294","prefix":"Loading annotations...4.4 连续与缺失值","suffix":"\" . 85 对属性\"密度\"在决策树学习开始时?根结点包含的 1"}]}]}
>```
>%%
>*%%PREFIX%%Loading annotations...4.4 连续与缺失值%%HIGHLIGHT%% ==例如在父结点上使用了"密度';;;0.381" ，不会禁止在子结点上使用"密度运0.294== %%POSTFIX%%" . 85 对属性"密度"在决策树学习开始时?根结点包含的 1*
>%%LINK%%[[#^s5zez0qsjng|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^s5zez0qsjng


>%%
>```annotation-json
>{"created":"2025-11-13T11:26:24.789Z","updated":"2025-11-13T11:26:24.789Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":102098,"end":102137},{"type":"TextQuoteSelector","exact":"与离散属性不同，若当前结点划分属性为连续属性?该属性还可作为其后代结点的划分属","prefix":".8 在西瓜数据集 3.0上基于信息增益生成的决策树需注意的是，","suffix":"性.4 .4 .2 缺失值处理现实任务中常会遇到不完整样本，即样"}]}]}
>```
>%%
>*%%PREFIX%%.8 在西瓜数据集 3.0上基于信息增益生成的决策树需注意的是，%%HIGHLIGHT%% ==与离散属性不同，若当前结点划分属性为连续属性?该属性还可作为其后代结点的划分属== %%POSTFIX%%性.4 .4 .2 缺失值处理现实任务中常会遇到不完整样本，即样*
>%%LINK%%[[#^vyamfjv4u9g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vyamfjv4u9g


>%%
>```annotation-json
>{"created":"2025-11-13T11:26:56.313Z","updated":"2025-11-13T11:26:56.313Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":102762,"end":102817},{"type":"TextQuoteSelector","exact":" 如何在属性值缺失的情况 F进行划分属性选择?(2) 给定划分属性?若样本在该属性上的值缺失，如何对样本进行划","prefix":"『L唱Bi咱FI牛唱EA咱E41EL我们需解决两个问题: (1)","suffix":"分?给定训练集 D 和属性 α，令 D 表示 D 中在属性 α上"}]}]}
>```
>%%
>*%%PREFIX%%『L唱Bi咱FI牛唱EA咱E41EL我们需解决两个问题: (1)%%HIGHLIGHT%% ==如何在属性值缺失的情况 F进行划分属性选择?(2) 给定划分属性?若样本在该属性上的值缺失，如何对样本进行划== %%POSTFIX%%分?给定训练集 D 和属性 α，令 D 表示 D 中在属性 α上*
>%%LINK%%[[#^4v5hfl14pb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^4v5hfl14pb


>%%
>```annotation-json
>{"created":"2025-12-15T06:56:28.783Z","updated":"2025-12-15T06:56:28.783Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":113605,"end":113804},{"type":"TextQuoteSelector","exact":"可以证明 [Minsky and Papert, 1969]，若两类模式是线性可分的，即存在一个线性超平面能将它们分开，如图5叫a)-(c) 所示，则感知在fl的学习过程一定会收敛(converge) 而求得适当的权向量 ω = (W1; 叫;... ;Wn+1); 否则感知机学习过程将会发生振蔼(fluctuation) ， ω难以稳定下来，不能求得合适解，例如感知机甚至不能解决如图 5剧d)","prefix":"问题都是线性可分(linearly separable)的问题.","suffix":" 所示的\"非线性可分\"意味着用线性起平面无法划分 异或这样简单的"}]}]}
>```
>%%
>*%%PREFIX%%问题都是线性可分(linearly separable)的问题.%%HIGHLIGHT%% ==可以证明 [Minsky and Papert, 1969]，若两类模式是线性可分的，即存在一个线性超平面能将它们分开，如图5叫a)-(c) 所示，则感知在fl的学习过程一定会收敛(converge) 而求得适当的权向量 ω = (W1; 叫;... ;Wn+1); 否则感知机学习过程将会发生振蔼(fluctuation) ， ω难以稳定下来，不能求得合适解，例如感知机甚至不能解决如图 5剧d)== %%POSTFIX%%所示的"非线性可分"意味着用线性起平面无法划分 异或这样简单的*
>%%LINK%%[[#^j80bjz975cp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^j80bjz975cp


>%%
>```annotation-json
>{"created":"2025-12-15T06:57:32.220Z","updated":"2025-12-15T06:57:32.220Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":114301,"end":114366},{"type":"TextQuoteSelector","exact":"每层神经元与下层神经元全互连 3 神经元之间不存在同层连接， 也不存在跨层连接 . 这样的神经网络结构通常称为\"多层前馈神经网络 ","prefix":" .4史一般的，常见的神经网络是形如图 5 .6所示的层级结构，","suffix":"\" (multi-laye r  feedforwar d  n"}]}]}
>```
>%%
>*%%PREFIX%%.4史一般的，常见的神经网络是形如图 5 .6所示的层级结构，%%HIGHLIGHT%% ==每层神经元与下层神经元全互连 3 神经元之间不存在同层连接， 也不存在跨层连接 . 这样的神经网络结构通常称为"多层前馈神经网络== %%POSTFIX%%" (multi-laye r  feedforwar d  n*
>%%LINK%%[[#^cjzq34iycrw|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^cjzq34iycrw


>%%
>```annotation-json
>{"created":"2025-12-15T07:05:01.451Z","updated":"2025-12-15T07:05:01.451Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":119811,"end":119878},{"type":"TextQuoteSelector","exact":"然而，如果误差函数具有多个局部极小?则不能保证找到的解是全局最小.对后一种情形?我们称参数寻优陷入了局部极小， 这显然不是我们所希望的","prefix":"最小107 E 图 5.10 全局最小与局部极小小就是全局最小;","suffix":" .在现实任务中，人们常采用以下策略来试图 \"四t出\"局部极小，"}]}]}
>```
>%%
>*%%PREFIX%%最小107 E 图 5.10 全局最小与局部极小小就是全局最小;%%HIGHLIGHT%% ==然而，如果误差函数具有多个局部极小?则不能保证找到的解是全局最小.对后一种情形?我们称参数寻优陷入了局部极小， 这显然不是我们所希望的== %%POSTFIX%%.在现实任务中，人们常采用以下策略来试图 "四t出"局部极小，*
>%%LINK%%[[#^tphq3xwby8j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^tphq3xwby8j


>%%
>```annotation-json
>{"created":"2025-12-15T07:05:16.921Z","updated":"2025-12-15T07:05:16.921Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":119923,"end":119941},{"type":"TextQuoteSelector","exact":"以多组不同参数值初始化多个神经网络 ","prefix":"用以下策略来试图 \"四t出\"局部极小，从而进一步接近全局最小.·","suffix":"7 按标准方法训练后，取其中误差最小的解作为最终参数.这相当于从"}]}]}
>```
>%%
>*%%PREFIX%%用以下策略来试图 "四t出"局部极小，从而进一步接近全局最小.·%%HIGHLIGHT%% ==以多组不同参数值初始化多个神经网络== %%POSTFIX%%7 按标准方法训练后，取其中误差最小的解作为最终参数.这相当于从*
>%%LINK%%[[#^g46buqce7nk|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^g46buqce7nk


>%%
>```annotation-json
>{"created":"2025-12-15T07:05:25.693Z","updated":"2025-12-15T07:05:25.693Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":120025,"end":120061},{"type":"TextQuoteSelector","exact":" 使用 \"模拟*火\" (simulated  annealing) 技术","prefix":"入不同的局部极小从中进行选择有可能获得更接近全局最小的结果 .·","suffix":" [Aarts and  Korst,  1989]. 模拟退火"}]}]}
>```
>%%
>*%%PREFIX%%入不同的局部极小从中进行选择有可能获得更接近全局最小的结果 .·%%HIGHLIGHT%% ==使用 "模拟*火" (simulated  annealing) 技术== %%POSTFIX%%[Aarts and  Korst,  1989]. 模拟退火*
>%%LINK%%[[#^wmw1feawfh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wmw1feawfh


>%%
>```annotation-json
>{"created":"2025-12-15T07:05:49.511Z","updated":"2025-12-15T07:05:49.511Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":120282,"end":120309},{"type":"TextQuoteSelector","exact":" 遗传算法(genetic a lgorithms) ","prefix":"梯度仍可能不为零 3 这样就有机会跳出局部极小继续搜索 .此外，","suffix":" [Goldberg,  1989] 也常用来训练神经网络以更好"}]}]}
>```
>%%
>*%%PREFIX%%梯度仍可能不为零 3 这样就有机会跳出局部极小继续搜索 .此外，%%HIGHLIGHT%% ==遗传算法(genetic a lgorithms)== %%POSTFIX%%[Goldberg,  1989] 也常用来训练神经网络以更好*
>%%LINK%%[[#^cn91qmz4l5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^cn91qmz4l5


>%%
>```annotation-json
>{"created":"2025-12-15T07:05:52.524Z","updated":"2025-12-15T07:05:52.524Z","document":{"title":"%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","link":[{"href":"urn:x-pdf:8d557cc6a5343acc6e6e250406b3d0a5"},{"href":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf"}],"documentFingerprint":"8d557cc6a5343acc6e6e250406b3d0a5"},"uri":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","target":[{"source":"vault:/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E).pdf","selector":[{"type":"TextPositionSelector","start":120183,"end":120191},{"type":"TextQuoteSelector","exact":"使用随机梯度下降","prefix":"\"的概率要随着时间的推移而逐渐降低 7 从而保证算法稳定 .· ","suffix":".与标准梯度下降法精确计算梯度不同， 随机梯度下降法在计算梯度时"}]}]}
>```
>%%
>*%%PREFIX%%"的概率要随着时间的推移而逐渐降低 7 从而保证算法稳定 .·%%HIGHLIGHT%% ==使用随机梯度下降== %%POSTFIX%%.与标准梯度下降法精确计算梯度不同， 随机梯度下降法在计算梯度时*
>%%LINK%%[[#^28wozix11qi|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^28wozix11qi

> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [chat.zchat.tech](https://chat.zchat.tech/c/693fe6c2-db58-8330-964b-58b64cb02898)

> ChatGPT 是一款供日常使用的 AI 聊天机器人。与最先进的 AI 模型互动，探索创意、解决问题以及提升学习效率。

下面我按“零基础本科生”的节奏，把 **ART 网络、SOM 网络、级联相关网络（Cascade-Correlation）、Elman 网络、Boltzmann 机** 讲清楚。你会看到它们其实分别代表了神经网络里几种很典型的思想：**竞争学习（无监督聚类）**、**拓扑保持映射**、**网络结构自增长**、**循环/时序记忆**、**能量模型/生成模型**。

* * *

先打地基：你必须先懂的 3 个关键词
------------------

### 1）监督 vs 无监督

*   **监督学习**：输入 x 之外，还有标签 y，目标是预测 y（分类/回归）。
    
*   **无监督学习**：只有输入 x，目标是发现结构（聚类、降维、表示学习）。
    

### 2）“竞争学习”（competitive learning）

一堆输出神经元在“抢”同一个输入样本：

*   **只有一个/少数神经元赢**（winner-take-all 或 winner-take-most）
    
*   赢的那个神经元的权重向量会更像当前输入（“学到”这个样本）
    

这正是 **ART 和 SOM** 的核心。

### 3）“原型/权重向量” = “这个类别/节点的代表”

很多经典无监督网络（ART/SOM）里，每个输出节点都有一个权重向量 w，它就像“该节点代表的典型样本”。  
输入 x 来了，就看哪个 w 跟它最像（距离最小 / 相似度最大）。

* * *

1. ART 网络（Adaptive Resonance Theory，自适应谐振理论）
============================================

它想解决什么问题？
---------

一个经典矛盾叫：**稳定性—可塑性困境**（stability–plasticity dilemma）

*   **可塑性**：能不断学习新知识（新类别）
    
*   **稳定性**：学新知识时别把旧知识忘光
    

ART 的设计目标就是：**在线（增量）学习 + 不灾难性遗忘 + 自动增类**。

* * *

ART 的直觉：像“带门槛的动态聚类”
-------------------

每来一个输入 x：

1.  输出层（类别层）里所有“类别神经元”先**竞争**
    
2.  找到最像 x 的类别原型 wj​
    
3.  但它不一定立刻收下 x，还要做一个 **“警戒/识别阈值测试”**（vigilance test）：
    
    *   **像得足够像**（相似度 ≥ 阈值 ρ）：就把 x 归到这个类，并更新这个类的原型（学习）
        
    *   **不够像**：拒收这个类，换下一个候选类；如果都不合格，就 **创建一个新类别节点**
        

> 你教材里说的“识别阈值/识别阈值参数”，就是这个 ρ。

* * *

最关键的超参数：警戒参数 ρ
--------------

*   ρ **越高**：要求“必须非常像才算同类”  
    → 类别数会变多（更细分）
    
*   ρ **越低**：宽松  
    → 类别数变少（更粗糙）
    

这就是 ART 的“可控聚类粒度”。

* * *

ART 的训练流程（你可以记成 5 步）
--------------------

对每个输入向量 x：

1.  **计算相似度/选择函数**：每个类别 j 得到一个匹配分数 Tj​
    
2.  **赢家通吃**：选 j\*=argmaxj​Tj​
    
3.  **警戒测试**：检查 x 与 wj\*​ 是否足够相似（≥ ρ）
    
4.  若通过：**谐振（resonance）** → 更新 wj\*​（让它更接近 x）
    
5.  若不通过：**reset** 这个类别，换下一个；若都不通过：**新建类别**
    

* * *

ART 家族你只要先认识这几个名字
-----------------

*   **ART1**：输入是二值（0/1）
    
*   **ART2**：输入是连续值
    
*   **Fuzzy ART**：最常用的一类，把输入限制在 [0,1]，用“模糊与/最小值”做匹配与更新
    
*   **ARTMAP**：带监督版本（用于分类）
    

* * *

ART 的优缺点（考试很爱考）
---------------

✅ 优点

*   **增量学习 / 在线学习**：数据一个个来都能学
    
*   能自动决定“要不要开新类”（靠 ρ 控制）
    
*   相对不容易灾难性遗忘（比普通在线聚类更稳）
    

⚠️ 缺点

*   **对参数敏感**（尤其 ρ）
    
*   **对样本顺序敏感**（先来后到会影响形成的类别）
    
*   噪声大/阈值高时可能产生很多小类
    

* * *

2. SOM 网络（Self-Organizing Map，自组织映射）
====================================

它想解决什么问题？
---------

SOM 主要做两件事：

1.  **聚类/原型学习**（像 k-means 的一种神经网络形式）
    
2.  **高维数据降到 2D 网格并尽量保持邻近关系**（可视化神器）
    

一句话：**把高维数据“铺”到二维地图上**，让相似样本落在相近位置。

* * *

结构长什么样？
-------

*   输入层：维度 = 特征维度 d
    
*   输出层：一个二维网格（比如 10×10 个神经元）
    
*   每个输出神经元 i 也有一个权重向量 wi​∈Rd
    

* * *

SOM 的训练直觉：不止赢家学，赢家邻居也学
----------------------

给一个样本 x：

1.  找到与 x 最像的神经元（距离最小）：
    

BMU=argimin​∥x−wi​∥

BMU = best matching unit（最佳匹配单元），也就是“赢家”。

2.  更新 BMU 及其邻域（在网格上离 BMU 近的那些节点也更新）：
    

*   BMU 学得最多
    
*   邻居学得少一点
    
*   距离很远的节点几乎不动
    

3.  训练过程中：
    

*   学习率 α(t) 逐渐变小
    
*   邻域半径 σ(t) 逐渐变小  
    一开始“大家一起动”，后面“只微调赢家附近”。
    

* * *

SOM 你需要牢牢记住的关键词
---------------

*   **BMU（赢家）**
    
*   **邻域函数**（决定谁跟着赢家一起学、学多少）
    
*   **拓扑保持**（相似输入 → 映射到网格上相近位置）
    

* * *

SOM 的优缺点
--------

✅ 优点

*   很适合 **可视化高维数据**（聚类结构直观看出来）
    
*   无监督，训练逻辑简单
    
*   输出网格有“地理意义”，相近节点代表相似模式
    

⚠️ 缺点

*   超参数（网格大小、学习率衰减、邻域衰减）需要经验
    
*   大数据训练可能慢
    
*   不是严格概率模型，解释性更多靠可视化经验（如 U-matrix）
    

* * *

3. 级联相关网络（Cascade-Correlation Network，级联相关神经网络）
===============================================

> 这类网络很重要，因为它代表一种思想：**网络结构不是先固定，而是边学边长出来**。

它想解决什么问题？
---------

传统前馈网络（MLP）训练前就要决定：

*   几层？
    
*   每层多少隐藏单元？
    

级联相关网络的想法是：**从很小的网络开始，性能不够就增加隐藏节点**，直到够用为止。

* * *

核心机制（强烈建议你背下来）
--------------

它的训练是“分阶段”的：

### 阶段 A：先训练现有网络（不加新节点）

*   让输出误差尽量小（常规梯度下降/BP）
    

### 阶段 B：加一个新的隐藏节点（或一组）

*   新节点的输入连接通常来自：
    
    *   所有输入节点
        
    *   以及之前已经加入的隐藏节点
        
*   训练新节点时，不是直接最小化输出误差，而是让新节点的输出和“当前残差误差”**相关性最大**  
    （这就是名字里的 correlation）
    

### 阶段 C：把新节点“冻住”（冻结其输入权重）

*   新隐藏节点的输入权重固定不再改
    
*   然后只训练输出层权重，让网络利用新特征降低误差
    

重复 A→B→C，网络像“搭积木”一样逐步变复杂。

* * *

你需要理解的直觉
--------

*   每次加的隐藏节点都是在“解释当前模型还解释不了的那部分误差”
    
*   冻结旧节点的权重让训练更稳定、更快（但也带来局限）
    

* * *

优缺点
---

✅ 优点

*   **自动决定隐藏单元数量**（减少结构设计难题）
    
*   训练通常比较快（每次只加一点、并冻结一部分）
    
*   对小中型任务很有用（尤其在早期神经网络时代）
    

⚠️ 缺点

*   **可能过拟合**（一直加节点直到误差很小）
    
*   冻结权重意味着后期无法“全局再调整”，可能错过更优解
    
*   现代深度学习很少直接用它，但“逐步增长/结构搜索”的思想还在
    

* * *

4. Elman 网络（Elman Network，简单循环网络 SRN）
=====================================

它想解决什么问题？
---------

普通前馈网络只看当前输入 x，没有“记忆”。  
但很多任务需要利用历史信息：

*   时间序列预测
    
*   语音/文本序列（上下文）
    
*   控制系统
    

Elman 网络是最经典的**循环神经网络（RNN）**之一：它让网络能“记住上一时刻的隐藏状态”。

* * *

结构：多了一个“上下文/延迟”通道
-----------------

关键是：**隐藏层在时刻 t−1 的输出，会反馈到时刻 t 的输入端**（更准确说：作为隐藏层的额外输入）。

你可以把它写成最标准的 RNN 形式：

ht​=f(Wx​xt​+Wh​ht−1​+b) yt​=g(Wy​ht​)

*   ht​：隐藏状态（记忆）
    
*   Wh​：循环权重（让过去影响现在）
    

* * *

训练：时间上的 BP（BPTT）
----------------

因为同一套参数在时间上重复使用，所以要把网络“在时间上展开”，对 t,t−1,t−2... 反向传播，这叫：

*   **BPTT（Backpropagation Through Time）**  
    实践中常用 **截断 BPTT**（只回传最近 k 步），否则太慢。
    

* * *

优缺点（也很常考）
---------

✅ 优点

*   能处理序列、有短期记忆
    
*   结构简单，容易理解 RNN 的本质
    

⚠️ 缺点

*   容易出现 **梯度消失/爆炸**（长依赖学不动）
    
*   现在工业上更多用 **LSTM/GRU**，更现代则是 **Transformer**
    
*   但 Elman 是理解“循环记忆”的入门必经之路
    

* * *

5. Boltzmann 机（Boltzmann Machine）与受限 Boltzmann 机（RBM）
=====================================================

它想解决什么问题？
---------

Boltzmann 机属于 **能量模型（Energy-Based Model）/生成模型**：

*   它不是直接做分类回归
    
*   而是学习数据的概率分布：学会“什么样的样本更像训练数据”
    
*   学完后可以：
    
    *   给样本打分（像不像数据分布）
        
    *   补全缺失（推断）
        
    *   甚至生成新样本
        

* * *

核心思想：用“能量”定义概率
--------------

每个神经元是随机的（常见是二值 0/1），网络状态记为 s。  
定义一个能量函数（你教材里也给了类似形式）：

*   能量低 → 概率高
    
*   能量高 → 概率低
    

典型形式：

P(s)=Ze−E(s)​

*   Z 是配分函数（归一化常数），计算非常贵（因为要对所有状态求和）
    

* * *

训练为什么难？
-------

最大问题：要学参数，就要计算两种期望：

*   **数据分布下的统计量**（正相）
    
*   **模型自己分布下的统计量**（负相）
    

而“模型自己分布”要靠 **采样** 才能估计，训练很慢。

这也是为什么原始 Boltzmann 机不那么常用。

* * *

RBM（Restricted Boltzmann Machine）是关键改进
--------------------------------------

RBM 的结构：**可见层 v** 和 **隐藏层 h** 两层，且

*   可见层内部不连
    
*   隐藏层内部不连
    
*   只允许 v–h 之间连接（二部图）
    

好处巨大：条件概率变得简单：

*   给定 v，各个 hj​ 相互独立
    
*   给定 h，各个 vi​ 相互独立  
    因此可以“整层并行采样”（Gibbs sampling 变快）
    

* * *

CD（Contrastive Divergence，对比散度）训练（你教材也提到）
-----------------------------------------

这是训练 RBM 最经典的近似方法，CD-1 最常见：

对一个数据样本 v(0)：

1.  由 v(0) 采样 h(0)∼P(h∣v(0))
    
2.  用 h(0) 重构出 v(1)∼P(v∣h(0))
    
3.  再由 v(1) 采样 h(1)
    
4.  更新权重（直觉：让真实数据的“共现”更强，让重构样本的“共现”更弱）：
    

ΔW∝v(0)(h(0))T−v(1)(h(1))T

这就是“正相 - 负相”。

* * *

Boltzmann/RBM 的应用（历史与现在）
------------------------

*   历史上：**深度信念网络（DBN）**、无监督预训练（早期深度学习的重要路径）
    
*   现在：主流生成模型更多是 VAE/GAN/扩散模型，但 RBM 思想仍常用于：
    
    *   表示学习的理论学习
        
    *   协同过滤（推荐）
        
    *   能量模型相关研究
        

* * *

6. 一张“对比速查表”（帮你把它们分清）
=====================

<table data-start="5685" data-end="6165" class="w-fit min-w-(--thread-content-width)"><thead data-start="5685" data-end="5724"><tr data-start="5685" data-end="5724"><th data-start="5685" data-end="5690" data-col-size="sm">模型</th><th data-start="5690" data-end="5697" data-col-size="sm">学习类型</th><th data-start="5697" data-end="5704" data-col-size="sm">核心机制</th><th data-start="5704" data-end="5711" data-col-size="sm">典型用途</th><th data-start="5711" data-end="5724" data-col-size="sm">你最该记住的关键词</th></tr></thead><tbody data-start="5747" data-end="6165"><tr data-start="5747" data-end="5843"><td data-start="5747" data-end="5757" data-col-size="sm"><strong data-start="5749" data-end="5756">ART</strong></td><td data-start="5757" data-end="5766" data-col-size="sm">无监督/在线</td><td data-start="5766" data-end="5797" data-col-size="sm">竞争 + 警戒阈值 <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math>ρ + 不够像就开新类</td><td data-start="5797" data-end="5817" data-col-size="sm">增量聚类、在线分类（ARTMAP）</td><td data-start="5817" data-end="5843" data-col-size="sm">稳定-可塑性、vigilance、reset</td></tr><tr data-start="5844" data-end="5911"><td data-start="5844" data-end="5854" data-col-size="sm"><strong data-start="5846" data-end="5853">SOM</strong></td><td data-start="5854" data-end="5860" data-col-size="sm">无监督</td><td data-start="5860" data-end="5882" data-col-size="sm">BMU + 邻域更新 + 拓扑保持映射</td><td data-start="5882" data-end="5894" data-col-size="sm">可视化、聚类、降维</td><td data-start="5894" data-end="5911" data-col-size="sm">BMU、邻域函数、二维网格</td></tr><tr data-start="5912" data-end="5986"><td data-start="5912" data-end="5923" data-col-size="sm"><strong data-start="5914" data-end="5922">级联相关</strong></td><td data-start="5923" data-end="5928" data-col-size="sm">监督</td><td data-start="5928" data-end="5958" data-col-size="sm">从小到大生长网络；新节点最大化与残差误差的相关性；冻结</td><td data-start="5958" data-end="5970" data-col-size="sm">自动确定隐藏单元数</td><td data-start="5970" data-end="5986" data-col-size="sm">结构自增长、相关性、冻结</td></tr><tr data-start="5987" data-end="6065"><td data-start="5987" data-end="5999" data-col-size="sm"><strong data-start="5989" data-end="5998">Elman</strong></td><td data-start="5999" data-end="6008" data-col-size="sm">监督/自监督</td><td data-start="6008" data-end="6023" data-col-size="sm">隐藏状态反馈（短期记忆）</td><td data-start="6023" data-end="6037" data-col-size="sm">时序预测、序列建模入门</td><td data-start="6037" data-end="6065" data-col-size="sm">context、<math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math>ht−1​、BPTT</td></tr><tr data-start="6066" data-end="6165"><td data-start="6066" data-end="6088" data-col-size="sm"><strong data-start="6068" data-end="6087">Boltzmann / RBM</strong></td><td data-start="6088" data-end="6097" data-col-size="sm">无监督/生成</td><td data-start="6097" data-end="6125" data-col-size="sm">能量函数定义概率；采样学习；RBM + CD 加速</td><td data-start="6125" data-end="6138" data-col-size="sm">表示学习、生成、推荐</td><td data-start="6138" data-end="6165" data-col-size="sm">energy、Gibbs、CD、RBM 二部图</td></tr></tbody></table>

* * *

7. 给你一个“学习顺序建议”（最省脑）
====================

如果你真是零基础，建议按这个顺序理解：

1.  **SOM**：最直观，能把“竞争学习”理解透
    
2.  **ART**：理解“阈值 + 动态增类 + 在线学习”
    
3.  **Elman**：进入“时序/记忆”
    
4.  **Boltzmann/RBM**：进入“概率与能量模型”
    
5.  **级联相关**：理解“结构自适应/构造式学习”
    

* * *

8. 小练习（你做完就真懂了）
---------------

你可以不写代码也能练：

1.  **ART 思考题**：  
    如果把 vigilance ρ 从 0.3 调到 0.9，你预期类别数怎么变？为什么？
    
2.  **SOM 思考题**：  
    如果邻域半径一直不缩小，会发生什么？（提示：地图会不会“定型”？）
    
3.  **Elman 思考题**：  
    为什么 Elman 能处理序列，而普通 MLP 不行？（提示：ht−1​ 的作用）
    
4.  **RBM 思考题**：  
    “对比散度”为什么叫“对比”？它在对比什么？
    

* * *

如果你愿意，我可以继续用一个 **二维玩具数据集**（比如 3 个簇的点）带你手算/半手算：

*   ART 如何决定“开新类”
    
*   SOM 如何把点映射到网格上  
    并且把关键超参数（ρ、学习率、邻域半径）对结果的影响做成直观例子。你更想先看 **ART** 还是 **SOM** 的具体演示？